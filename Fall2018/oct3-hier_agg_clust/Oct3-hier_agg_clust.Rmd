---
title: "Hierarchical clustering"
author: "Evan"
date: "10/2/2018"
output: 
  html_document: 
    number_sections: yes
    toc: yes
    toc_float: yes
---

```{r set-options, echo = FALSE, cache = FALSE}
options(width = 140)
```

# Hierarchical agglomerative clustering
Hierarchical agglomerative clustering is a "bottom-up" method of clustering. Each observation begins as its own cluster and forms clusters with like items as it moves up the hierarchy. That is, all leaves are their own clusters to begin with and form clusters as we move up the trunk and various branches are formed.  

Distance and cluster method information are usually displayed at the bottom of the graph, while the vertical axis displays the height, which refers to the distance between two clusters. We are not concerned as much with distances along the horizontal axis. We can also "cut" the dendrogram to specify a number of clusters, which is similar to defining _k_ in k-means clustering (which is also equally problematic).  

In a real-life research situation, you will likely want to scale the data. However, raw data are used in this example. 
# Package installation
```{r}
if (FALSE) {
  # Run this line manually (once) to install the necessary packages.
  # Install packages from CRAN:
  install.packages(c("ape", "pvclust", "mclust"))
}

# fancy dendrogram options
library(ape)
# dendrograms with p-values
library(pvclust)
# model-based clustering
library(mclust)
```

# Load data
```{r}
data(mtcars)
?mtcars
```

Start by using the `hclust` built-in function from [{stats}](https://www.rdocumentation.org/packages/stats/versions/3.5.1). `hclust` prefers a dissimilarity matrix via the `dist` function, thus it plots rows as opposed to columns like the methods further below. 

# The `hclust` built-in function
```{r}
# See the help files
?hclust

# Create distance matrix
mtcars_dist = dist(mtcars, method = "euclidean")

# Fit hclust_model
system.time({
  hclust_model = hclust(mtcars_dist, method = "complete")
  })

# Plot hclust_model dendrogram
plot(hclust_model, hang = -1)
```

Data are visualized in dendrograms, or branching tree-like structures similar to decision trees, albeit with less information displayed at each node. The most similar items are found lower in the dendrogram and fuse into $n-1$ clusters as we move up the tree; the next two items to fuse into a cluster produce $n-2$ clusters and so on as we move up the tree until there is just one overarching cluster. Thus, clusters become more inclusive as we move up the hierarchy.  

Dissimilarity is applied not just to single observations, but to groups as well (linkage). Thus the "Cadillac Fleetwood / Lincoln Continental" cluster " cluster fuses with "Chrysler Imperial" instead of "Maserati Bora" or something else.  

You can also cut the tree to see how the tree varies:
```{r}
# If we want only 5 clusters, for example (must be a number between 1-32, since mtcars has only 32 observations:
cutree(hclust_model, 5) 
```

# The `ape` package

The [`ape` package](https://cran.r-project.org/web/packages/ape/index.html) provides some great functionality for constructing and plotting clusters:
```{r}
library(ape)
# various plots
plot(as.phylo(hclust_model))
plot(as.phylo(hclust_model), type = "cladogram")
plot(as.phylo(hclust_model), type = "unrooted")

# radial plot
colors = c("red", "orange", "blue", "green", "purple")
clus5 = cutree(hclust_model, 5)
plot(as.phylo(hclust_model), type = "fan", tip.color = colors[clus5], lwd = 2, cex = 1)
```

> NOTE: the color settings for the radial plot apply to the other ape plots as well.  

# The `pvclust` package
The [pvclust](http://stat.sys.i.kyoto-u.ac.jp/prog/pvclust/) package offers a straightfoward way to perform hierarchical agglomerative clustering of columns with two types of p-values at each split: approximately unbiased **(AU)** and bootstrap probability **(BP)**. 
```{r}
library(pvclust)
# Cluster features

# Ward's method: minimum variance between clusters
system.time({
  pvclust_model_ward = pvclust(mtcars, 
                          method.hclust = "ward.D",
                          method.dist = "euclidean",
                          nboot = 1000, parallel = T)
  })

plot(pvclust_model_ward)

# pvrect will draw rectangles around clusters with high or low p-values
pvrect(pvclust_model_ward, alpha = 0.95)
```

### Compare different dissimilarity measures
```{r}
# Complete linkage: largest intercluster difference
system.time({
  pvclust_model_complete = pvclust(mtcars, 
                          method.hclust = "complete",
                          method.dist = "euclidean",
                          nboot = 1000, parallel = T)
})

# Single linkage: smallest intercluster difference
system.time({
  pvclust_model_single = pvclust(mtcars, 
                          method.hclust = "single",
                          method.dist = "euclidean",
                          nboot = 1000, parallel = T)
})

# Average linkage: mean intercluster difference
system.time({
  pvclust_model_average = pvclust(mtcars, 
                          method.hclust = "average",
                          method.dist = "euclidean",
                          nboot = 1000, parallel = T)
})

# View summaries
pvclust_model_ward
pvclust_model_complete
pvclust_model_single
pvclust_model_average

# Plot Euclidean distance linkages
par(mfrow = c(2,2))
plot(pvclust_model_ward, main = "Ward", xlab = "", sub = "")
pvrect(pvclust_model_ward)
plot(pvclust_model_complete, main = "Complete", xlab = "", sub = "")
pvrect(pvclust_model_complete)
plot(pvclust_model_single, main = "Single", xlab = "", sub = "")
pvrect(pvclust_model_single)
plot(pvclust_model_average, main = "Average", xlab = "", sub = "")
pvrect(pvclust_model_average)
par(mfrow = c(1,1))
```

### View standard error plots:
```{r}
par(mfrow=c(2,2))
seplot(pvclust_model_ward, main = "Ward")
seplot(pvclust_model_complete, main = "Complete")
seplot(pvclust_model_single, main = "Single")
seplot(pvclust_model_average, main = "Average")
par(mfrow=c(1,1))
```

# Going further - the `mclust` package
The [`mclust`](https://cran.r-project.org/web/packages/mclust/index.html) package provides "Gaussian finite mixture models fitted via EM algorithm for model-based clustering, classification, and density estimation, including Bayesian regularization, dimension reduction for visualisation, and resampling-based inference."
```{r}
library(mclust)

# Fit model
mclust_model = Mclust(mtcars)

# View various plots
plot(mclust_model, what = "BIC") 
plot(mclust_model, what = "classification")
plot(mclust_model, what = "uncertainty")
plot(mclust_model, what = "density")
```

### Return best performing model
```{r}
summary(mclust_model)
```

### Cross-validated mclust
```{r}
# sort mpg in decreasing order
mtcars = mtcars[order(-mtcars$mpg),]
mtcars 

# create a binary factor variable from mpg: "less than 20mpg" and "greater than 20mpg"
mtcars$class = cut(mtcars$mpg, 
                   breaks = c(0, 20, 40),
                   levels = c(1, 2),
                   labels = c("less than 20mpg", "greater than 20mpg"))
mtcars

# define our predictors (X) and class labels (class)
X = mtcars[ , -12]
class = mtcars$class

# fit the model (EEE covariance structure, basically the same as linear discriminant analysis)
mclust_model2 = MclustDA(X, class = class, modelType = "EDDA", modelNames = "EEE")

# cross-validate!
set.seed(1)
cv_mclust = cvMclustDA(mclust_model2, nfold = 20)

# View cross-validation error and standard error of the cv error
cv_mclust[c("error", "se")]
```

References and resources:  
- [Quick-R: Cluster Analysis](https://www.statmethods.net/advstats/cluster.html)  
- [James et al. Introduction to Statistical Learning, pp. 390-401](https://www-bcf.usc.edu/~gareth/ISL/)  
- [pvclust](http://stat.sys.i.kyoto-u.ac.jp/prog/pvclust/)  
- [STHDA: Beautiful dendrogram visualizations](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning)  
- [Gaston Sanchez: Visualizing Dendrograms in R](https://rpubs.com/gaston/dendrograms)  
- [Analysis of Phylogenetics and Evolution](http://ape-package.ird.fr/)  
- [A Quick Tour of mclust](https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html)  
- [mclust vignette (from 2012, but more detailed)](https://www.stat.washington.edu/sites/default/files/files/reports/2012/tr597.pdf)  
- A very [useful walkthrough](https://quantdev.ssri.psu.edu/sites/qdev/files/Unsupervised_Machine_Learning_The_mclust_Package_and_others.html) by Christian Lopez  
- [MoEClust:](https://cran.r-project.org/web/packages/MoEClust/vignettes/MoEClust.html) Gaussian Parsimonious Clustering Models with Gating and Expert Network Covariates  
- See the [cluster](https://cran.r-project.org/web/packages/cluster/cluster.pdf) R package to learn more about agnes, clara, daisy, diana, fanny, flower, mona, and pam cluster methods!  


